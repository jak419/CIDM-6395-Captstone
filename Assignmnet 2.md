# What I Know
In the first course I took in this section, I developed a solid foundation in SQL database design, data modeling, and database optimization. My experience includes designing conceptual, logical, and physical data models, ensuring efficient database normalization, and improving query performance through indexing and query optimization. Data modeling progresses through conceptual, logical, and physical stages to ensure that business requirements are accurately translated into efficient database structures. Conceptual model focuses on high-level entities, the logical model details attributes and relationships, and the physical model implements the design within a specific database system. This structured approach ensures scalability, maintainability, and clarity throughout database development.
 I also gained experience with relational database management systems. I used MySQL for my project portion of this of course. However, I also took a couple of tutorials that gave me exposure to other relational databases such as SQL Server, and PostgreSQL. My exposure to data modeling included designing star and snowflake schemas for data warehousing. learning star and snowflake schemas was essential because these data warehouse designs optimize query performance and storage efficiency. Star schemas simplify queries by having a central fact table connected to dimension tables, making it suitable for OLAP operations. Snowflake schemas normalize dimension tables, reducing data redundancy, though they can be more complex. Database normalization is crucial for reducing redundancy, improving data integrity, and enhancing query performance. It ensures data is logically stored and avoids anomalies during data operations.  I built data pipelines using ETL tools such as Informatica ensuring efficient data extraction, transformation, and loading processes. Hands-on experience with database normalization, indexing, and stored procedures strengthened my ability to design optimized databases that ensure data integrity and performance.
I took Data Mining Methods for the second course in this section. In this course I acquired extensive knowledge in data mining methodologies, statistical analysis, and machine learning techniques. This course typically covers essential concepts such as data exploration, feature engineering, and model evaluation and was instructed in R. I became proficient with R data wrangling libraries like dplyr and ggplot2 for visualization. The tool I mainly used was RStudio. Another tool I became proficient with was RapidMiner. RapidMiner introduces users to advanced machine learning workflows, enabling them to design, implement, and compare algorithms such as clustering, classification, and regression without extensive coding.  It allows users to also learn data preprocessing techniques, including scaling, normalization, and handling missing values, preparing them for industry-level data analysis and predictive modeling challenges. My work involved preparing raw data for analysis, implementing classification algorithms such as Na√Øve Bayes and decision trees, and comparing machine learning models to identify the most accurate and efficient ones.

# Supporting Evidence
My GitHub repositories highlights my practical experience in data management. The Online Store Project demonstrates my ability to design and implement SQL databases for managing online store orders and inventory, with Power BI used for data visualization.  In Data mining methods I worked on a project which focused on developing a credit risk assessment model using data mining techniques to improve the accuracy and efficiency of credit risk evaluations by financial institutions. The project was developed using German Credit dataset from the UCI Machine Learning Repository, consisting of 1000 instances with numerical and categorical features, highlighting the importance of correctly identifying high-risk applicants due to associated costs. Machine learning models were built in R Studio and Rapid Miner, hyperparameter tuning, and selecting the best-performing model were performed.

# Weaknesses
While I have gained substantial knowledge in data management, there are areas where I need improvement. My experience with advanced database optimization techniques such as partitioning, indexing strategies, and query performance tuning needs further development. Additionally, I recognize that I need more hands-on practice with big data technologies like Hadoop and Spark. My knowledge of data governance, data security policies, and cloud-based data management tools such as Amazon RDS and Google BigQuery is still growing. Furthermore, while I have used ETL tools, mastering complex data integration workflows and automation is an area I aim to strengthen.

# What I Wish I Knew
While I have gained substantial skills, I recognize there are areas I wish I knew more about. Advanced machine learning algorithms such as ensemble methods (Random Forest, Gradient Boosting) and deep learning frameworks like TensorFlow and PyTorch are aspects I aim to explore. Additionally, I wish to deepen my understanding of cloud-based data engineering tools beyond Azure, such as AWS and Google Cloud. Gaining exposure to data ethics and governance would also enhance my ability to manage data responsibly. Lastly, enhancing my skills in handling unstructured data, including text and multimedia, is essential for tackling modern data challenges

# Future Applications
I acquired many skills in this portion of the curriculum. The skills acquired in Data Management are highly applicable to my current role and future career aspirations.  These courses have provided me with the technical foundation to design and maintain scalable data architectures, ensuring data quality and integrity in complex ecosystems. Moving forward, I will apply my expertise to build and optimize data warehouses, implement efficient ETL workflows, and leverage big data technologies like Hadoop and Spark. My ability to manage both structured and unstructured data will allow me to contribute to innovative solutions in business intelligence, machine learning, and cloud data management.
